{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **TRABAJO DE FIN DE UNIDAD 02**"
      ],
      "metadata": {
        "id": "EKXkZRkFgUIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Estudio comparativo de algoritmos de detección de rostros basados en HOG y CNN con análisis de técnicas de oclusión**\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "```\n",
        "Integrantes:\n",
        "- CHOQUE QUISPE JADYRA CH'ASKA - 204795\n",
        "- HANCCO CHAMPI FRAN ANTHONY - 204797\n",
        "- JALLO PACCAYA YASUMY MARICELY - 204799\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ewUHua4pfzd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. IMPORTAR DATOS**"
      ],
      "metadata": {
        "id": "bPziuwL5hflZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los datos fueron obtenidos del conjunto de datos CelebA a través de la biblioteca `kagglehub`, utilizando el identificador `\"jessicali9530/celeba-dataset\"`. Este conjunto de datos contiene imágenes de rostros de celebridades y sus respectivas etiquetas de atributos."
      ],
      "metadata": {
        "id": "GHqMrafAhyKH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRE9M_Jnvsjs",
        "outputId": "0d17b626-c5fd-43a6-bc2a-5817e9abe739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jessicali9530/celeba-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.33G/1.33G [00:47<00:00, 30.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jessicali9530/celeba-dataset/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carga de Imágenes y Atributos del Dataset CelebA**"
      ],
      "metadata": {
        "id": "HwOuTChhiQqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "# Ruta a las imágenes y a los atributos\n",
        "images_path = os.path.join(path, \"img_align_celeba\")\n",
        "attributes_path = os.path.join(path, \"list_attr_celeba.csv\")\n",
        "\n",
        "# Cargar los atributos de las imágenes\n",
        "attributes = pd.read_csv(attributes_path)\n",
        "\n",
        "# Cargar una muestra de imágenes\n",
        "sample_images = [cv2.imread(os.path.join(images_path, img)) for img in os.listdir(images_path)[:10]]\n"
      ],
      "metadata": {
        "id": "GtzY3Cukg1CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contenido del directorio principal**"
      ],
      "metadata": {
        "id": "wZJ6A3VZjM9F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDymqRFhqI19",
        "outputId": "a58456f2-5f9d-4d44-fcc0-4d1c86c46246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contenido del directorio principal: ['list_bbox_celeba.csv', 'img_align_celeba', 'list_eval_partition.csv', 'list_attr_celeba.csv', 'list_landmarks_align_celeba.csv']\n",
            "La carpeta img_align_celeba existe.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Listar archivos y carpetas en el directorio principal del dataset\n",
        "print(\"Contenido del directorio principal:\", os.listdir(path))\n",
        "\n",
        "# Comprobar si existe la carpeta img_align_celeba\n",
        "images_folder = os.path.join(path, \"img_align_celeba\")\n",
        "if os.path.exists(images_folder):\n",
        "    print(\"La carpeta img_align_celeba existe.\")\n",
        "else:\n",
        "    print(\"La carpeta img_align_celeba no existe. Verifica el nombre o la estructura de archivos.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpajTkt2vPbX",
        "outputId": "de0e2cda-9409-4ff0-80b7-5685aba20a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ruta especificada para images_path: /root/.cache/kagglehub/datasets/jessicali9530/celeba-dataset/versions/2/img_align_celeba\n",
            "Contenido de path: ['list_bbox_celeba.csv', 'img_align_celeba', 'list_eval_partition.csv', 'list_attr_celeba.csv', 'list_landmarks_align_celeba.csv']\n",
            "Contenido de img_align_celeba: ['img_align_celeba']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Imprimir la ruta completa a las imágenes\n",
        "print(\"Ruta especificada para images_path:\", images_path)\n",
        "\n",
        "# Listar el contenido de la carpeta principal\n",
        "print(\"Contenido de path:\", os.listdir(path))\n",
        "\n",
        "# Listar el contenido de la carpeta img_align_celeba si existe\n",
        "if os.path.exists(images_path):\n",
        "    print(\"Contenido de img_align_celeba:\", os.listdir(images_path)[:10])  # Muestra los primeros 10 elementos\n",
        "else:\n",
        "    print(\"La carpeta img_align_celeba no existe en la ubicación especificada.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 1. Importar Biblioteca Necesarias**"
      ],
      "metadata": {
        "id": "IhSDhwWCnJdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necesarias\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Para el procesamiento de datos\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Para el modelo MobileNetV2\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Para el modelo ResNet50\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Para evaluar el rendimiento\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "lfSsoTo7dn0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 2: Cargar y Preprocesar el Dataset CelebA**"
      ],
      "metadata": {
        "id": "mZDDoXcwnggi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(attributes.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eolk8_05h1k-",
        "outputId": "f412a063-c2db-44d7-a7f6-2f2f0c139989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['image_id', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',\n",
            "       'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose',\n",
            "       'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows',\n",
            "       'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair',\n",
            "       'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
            "       'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin',\n",
            "       'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns',\n",
            "       'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n",
            "       'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace',\n",
            "       'Wearing_Necktie', 'Young'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define paths\n",
        "images_path = os.path.join(path, \"img_align_celeba\", \"img_align_celeba\")\n",
        "attributes_path = os.path.join(path, \"list_attr_celeba.csv\")\n",
        "\n",
        "# Load image attributes\n",
        "attributes = pd.read_csv(attributes_path)\n",
        "\n",
        "# Create a function to label occlusions\n",
        "def label_occlusion(row):\n",
        "    if row['Eyeglasses'] == 1:\n",
        "        return \"Eyeglasses\"\n",
        "    elif row['Blurry'] == 1:\n",
        "        return \"Blurry\"\n",
        "    elif row['Wearing_Hat'] == 1:\n",
        "        return \"Hat\"\n",
        "    else:\n",
        "        return \"No Occlusion\"\n",
        "\n",
        "# Apply the function to create the 'label' column\n",
        "attributes['label'] = attributes.apply(label_occlusion, axis=1)\n",
        "\n",
        "# Show the first records to verify\n",
        "print(attributes.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBoMyNFwjp99",
        "outputId": "7c7a189c-4ae7-49e6-d2ee-da19a7a7f65a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     image_id  5_o_Clock_Shadow  Arched_Eyebrows  Attractive  Bags_Under_Eyes  \\\n",
            "0  000001.jpg                -1                1           1               -1   \n",
            "1  000002.jpg                -1               -1          -1                1   \n",
            "2  000003.jpg                -1               -1          -1               -1   \n",
            "3  000004.jpg                -1               -1           1               -1   \n",
            "4  000005.jpg                -1                1           1               -1   \n",
            "\n",
            "   Bald  Bangs  Big_Lips  Big_Nose  Black_Hair  ...  Smiling  Straight_Hair  \\\n",
            "0    -1     -1        -1        -1          -1  ...        1              1   \n",
            "1    -1     -1        -1         1          -1  ...        1             -1   \n",
            "2    -1     -1         1        -1          -1  ...       -1             -1   \n",
            "3    -1     -1        -1        -1          -1  ...       -1              1   \n",
            "4    -1     -1         1        -1          -1  ...       -1             -1   \n",
            "\n",
            "   Wavy_Hair  Wearing_Earrings  Wearing_Hat  Wearing_Lipstick  \\\n",
            "0         -1                 1           -1                 1   \n",
            "1         -1                -1           -1                -1   \n",
            "2          1                -1           -1                -1   \n",
            "3         -1                 1           -1                 1   \n",
            "4         -1                -1           -1                 1   \n",
            "\n",
            "   Wearing_Necklace  Wearing_Necktie  Young         label  \n",
            "0                -1               -1      1  No Occlusion  \n",
            "1                -1               -1      1  No Occlusion  \n",
            "2                -1               -1      1        Blurry  \n",
            "3                 1               -1      1  No Occlusion  \n",
            "4                -1               -1      1  No Occlusion  \n",
            "\n",
            "[5 rows x 42 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mostrar la columna 'label'**"
      ],
      "metadata": {
        "id": "TpEJpCQlp52b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar los primeros registros para verificar\n",
        "print(attributes[['image_id', 'label']].sample(20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMRuOn09Vfft",
        "outputId": "548da254-ca71-4bf3-bd8c-b58570a17250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          image_id         label\n",
            "104113  104114.jpg  No Occlusion\n",
            "94010   094011.jpg  No Occlusion\n",
            "78755   078756.jpg  No Occlusion\n",
            "36207   036208.jpg  No Occlusion\n",
            "140953  140954.jpg  No Occlusion\n",
            "53958   053959.jpg  No Occlusion\n",
            "91916   091917.jpg  No Occlusion\n",
            "124625  124626.jpg  No Occlusion\n",
            "24687   024688.jpg  No Occlusion\n",
            "201570  201571.jpg  No Occlusion\n",
            "80214   080215.jpg  No Occlusion\n",
            "27170   027171.jpg  No Occlusion\n",
            "23757   023758.jpg  No Occlusion\n",
            "106943  106944.jpg  No Occlusion\n",
            "164324  164325.jpg  No Occlusion\n",
            "124227  124228.jpg  No Occlusion\n",
            "171709  171710.jpg           Hat\n",
            "167415  167416.jpg  No Occlusion\n",
            "110380  110381.jpg           Hat\n",
            "37576   037577.jpg    Eyeglasses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **MODELO MOBILENETV2**"
      ],
      "metadata": {
        "id": "pxd8erW9yC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dividir el Conjunto de Datos en Entrenamiento y Validación**"
      ],
      "metadata": {
        "id": "TJrRFFlNBD5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir el conjunto de datos (80% entrenamiento, 20% validación)\n",
        "train_df, val_df = np.split(attributes.sample(frac=1, random_state=42), [int(0.8 * len(attributes))])\n",
        "\n",
        "# Verificar el tamaño de los conjuntos\n",
        "print(f\"Tamaño del conjunto de entrenamiento: {len(train_df)}\")\n",
        "print(f\"Tamaño del conjunto de validación: {len(val_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxq1SuVbBIIO",
        "outputId": "502306d3-9593-42d5-a5b2-e664ee739000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del conjunto de entrenamiento: 162079\n",
            "Tamaño del conjunto de validación: 40520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definir y Compilar el Modelo MobileNetV2**"
      ],
      "metadata": {
        "id": "NgyTgls3BDyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_mobilenetv2_model():\n",
        "    # Definir el modelo base\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Descongelar las últimas 20 capas para fine-tuning\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[:-20]:  # Congelar todas menos las últimas 20 capas\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Construir el modelo completo\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dropout(0.5),  # Dropout para reducir sobreajuste\n",
        "        Dense(128, activation='relu'),  # Aumentar el tamaño de esta capa\n",
        "        Dropout(0.3),  # Dropout adicional\n",
        "        Dense(6, activation='softmax')  # Cambiar a 6 clases si tienes 6 tipos\n",
        "    ])\n",
        "\n",
        "    # Compilar el modelo con el optimizador ajustado\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Crear el modelo con la configuración mejorada\n",
        "mobilenetv2_model = create_mobilenetv2_model()\n"
      ],
      "metadata": {
        "id": "dBeVL9r5eDgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb46e8d-e247-4d91-d3d5-624c960450d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "uGjeQkRa4hzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=5, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(factor=0.2, patience=3, min_lr=1e-6)\n",
        "]\n"
      ],
      "metadata": {
        "id": "azUiTvUw4v-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Entrenar MobileNetV2 Sin Datos de Oclusiones**"
      ],
      "metadata": {
        "id": "-6BAHtzmBoEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Filtrar solo las imágenes sin oclusiones\n",
        "train_data_no_occlusion = train_df[train_df['label'] == 'No Occlusion']\n",
        "val_data_no_occlusion = val_df[val_df['label'] == 'No Occlusion']\n",
        "\n",
        "# Crear generadores de datos\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)  # Normalización\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Crear generadores para el conjunto de entrenamiento y validación\n",
        "train_generator_no_occlusion = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_data_no_occlusion,\n",
        "    x_col='image_id',\n",
        "    y_col='label',\n",
        "    directory=images_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "\n",
        "val_generator_no_occlusion = val_datagen.flow_from_dataframe(\n",
        "    dataframe=val_data_no_occlusion,\n",
        "    x_col='image_id',\n",
        "    y_col='label',\n",
        "    directory=images_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVkFsnUNjUBy",
        "outputId": "e5cc4c8e-5fbf-4ca3-b051-2b74ba6bef66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 137697 validated image filenames belonging to 1 classes.\n",
            "Found 34410 validated image filenames belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo MobileNetV2 sin datos de oclusión\n",
        "history_no_occlusion = mobilenetv2_model.fit(\n",
        "    train_generator_no_occlusion,\n",
        "    validation_data=val_generator_no_occlusion,\n",
        "    epochs=5  # Ajusta el número de épocas según sea necesario\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hF0Tj0LYGtQ",
        "outputId": "71d60a08-5eb3-45ca-fbab-46aa8673f876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4304/4304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 48ms/step - accuracy: 0.9987 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/5\n",
            "\u001b[1m4304/4304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/5\n",
            "\u001b[1m4304/4304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/5\n",
            "\u001b[1m4304/4304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/5\n",
            "\u001b[1m4304/4304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizar y visualizar el rendimiento del modelo MobileNetV2"
      ],
      "metadata": {
        "id": "euvxczlR6FDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Entrenar MobileNetV2 Con Datos que incluyen Oclusiones**"
      ],
      "metadata": {
        "id": "4JXc2QF8Jemn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "crear y preparar generadores de datos para el entrenamiento y validación de un modelo de detección de rostros con oclusiones."
      ],
      "metadata": {
        "id": "44twKoaQ7YWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generadores de datos para el entrenamiento con oclusiones\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)  # Normalización\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Crear generadores para el conjunto de entrenamiento y validación con todas las clases de oclusión\n",
        "train_generator_with_occlusion = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='image_id',\n",
        "    y_col='label',\n",
        "    directory=images_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse'  # Configurado para múltiples clases\n",
        ")\n",
        "\n",
        "val_generator_with_occlusion = val_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    x_col='image_id',\n",
        "    y_col='label',\n",
        "    directory=images_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='sparse'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83tN1r9xEEvA",
        "outputId": "dfeaef42-0fd3-4fb2-d78f-80d5f911eaa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 162079 validated image filenames belonging to 4 classes.\n",
            "Found 40520 validated image filenames belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo MobileNetV2 con datos que incluyen oclusiones\n",
        "history_with_occlusion = mobilenetv2_model.fit(\n",
        "    train_generator_with_occlusion,\n",
        "    validation_data=val_generator_with_occlusion,\n",
        "    epochs=10  # Puedes ajustar el número de épocas según sea necesario\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqf-tAh2bzr6",
        "outputId": "56c6e82b-a1e7-4539-8a06-034a23388f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5065/5065\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 47ms/step - accuracy: 0.9241 - loss: 0.4592 - val_accuracy: 0.9461 - val_loss: 0.1514\n",
            "Epoch 2/10\n",
            "\u001b[1m5065/5065\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 45ms/step - accuracy: 0.9492 - loss: 0.1416 - val_accuracy: 0.9486 - val_loss: 0.1529\n",
            "Epoch 3/10\n",
            "\u001b[1m5065/5065\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 45ms/step - accuracy: 0.9550 - loss: 0.1222 - val_accuracy: 0.9503 - val_loss: 0.1380\n",
            "Epoch 4/10\n",
            "\u001b[1m5065/5065\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 45ms/step - accuracy: 0.9592 - loss: 0.1076 - val_accuracy: 0.9412 - val_loss: 0.1597\n",
            "Epoch 5/10\n",
            "\u001b[1m5065/5065\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 45ms/step - accuracy: 0.9632 - loss: 0.0954 - val_accuracy: 0.9508 - val_loss: 0.1546\n",
            "Epoch 6/10\n",
            "\u001b[1m5065/5065\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 45ms/step - accuracy: 0.9689 - loss: 0.0794 - val_accuracy: 0.9438 - val_loss: 0.1710\n",
            "Epoch 7/10\n",
            "\u001b[1m5065/5065\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 45ms/step - accuracy: 0.9746 - loss: 0.0657 - val_accuracy: 0.9390 - val_loss: 0.1967\n",
            "Epoch 8/10\n",
            "\u001b[1m5065/5065\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 45ms/step - accuracy: 0.9802 - loss: 0.0544 - val_accuracy: 0.9458 - val_loss: 0.2377\n",
            "Epoch 9/10\n",
            "\u001b[1m5065/5065\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 45ms/step - accuracy: 0.9828 - loss: 0.0462 - val_accuracy: 0.9482 - val_loss: 0.3078\n",
            "Epoch 10/10\n",
            "\u001b[1m5065/5065\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 44ms/step - accuracy: 0.9867 - loss: 0.0381 - val_accuracy: 0.9410 - val_loss: 0.2605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluar el Modelo MobileNetV2**"
      ],
      "metadata": {
        "id": "JsLW9-vgMzy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el modelo MobileNetV2 en conjunto de validación con oclusiones\n",
        "eval_with_occlusion_mobilenet = mobilenetv2_model.evaluate(val_generator_with_occlusion)\n",
        "print(f\"Precisión de MobileNetV2 con oclusiones: {eval_with_occlusion_mobilenet[1]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7veBu-o1LUTt",
        "outputId": "a60fe5b5-2955-43c3-c995-6d6c1c61a18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1267/1267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 35ms/step - accuracy: 0.9398 - loss: 0.2658\n",
            "Precisión de MobileNetV2 con oclusiones: 0.9410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Medir velocidad para MobileNetV2 en 10 imágenes de validación con oclusiones\n",
        "start_time = time.time()\n",
        "for i, (img, label) in enumerate(val_generator_with_occlusion):\n",
        "    if i >= 10:  # Limitar a 10 imágenes\n",
        "        break\n",
        "    mobilenetv2_model.predict(img)\n",
        "time_taken_mobilenet_with_occlusion = (time.time() - start_time) / 10\n",
        "print(f\"Tiempo promedio de detección de MobileNetV2 con oclusiones: {time_taken_mobilenet_with_occlusion:.4f} segundos\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gvOLjZlWZTN",
        "outputId": "91680947-7c59-4d02-a227-caf654eeede9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Tiempo promedio de detección de MobileNetV2 con oclusiones: 0.3536 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparar las métricas de entrenamiento y validación de dos modelos MobileNetV2 (uno entrenado con oclusiones y otro sin ellas) y analiza cuál modelo tiene mejor precisión de validación al final del entrenamiento"
      ],
      "metadata": {
        "id": "-jDo9N0m8UIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access training and validation metrics for both models\n",
        "train_accuracy_no_occlusion = history_no_occlusion.history['accuracy']\n",
        "val_accuracy_no_occlusion = history_no_occlusion.history['val_accuracy']\n",
        "train_loss_no_occlusion = history_no_occlusion.history['loss']\n",
        "val_loss_no_occlusion = history_no_occlusion.history['val_loss']\n",
        "\n",
        "train_accuracy_with_occlusion = history_with_occlusion.history['accuracy']\n",
        "val_accuracy_with_occlusion = history_with_occlusion.history['val_accuracy']\n",
        "train_loss_with_occlusion = history_with_occlusion.history['loss']\n",
        "val_loss_with_occlusion = history_with_occlusion.history['val_loss']\n",
        "\n",
        "# Print the results\n",
        "print(\"MobileNetV2 without Occlusion Data:\")\n",
        "print(\"Training Accuracy:\", train_accuracy_no_occlusion)\n",
        "print(\"Validation Accuracy:\", val_accuracy_no_occlusion)\n",
        "print(\"Training Loss:\", train_loss_no_occlusion)\n",
        "print(\"Validation Loss:\", val_loss_no_occlusion)\n",
        "\n",
        "\n",
        "print(\"\\nMobileNetV2 with Occlusion Data:\")\n",
        "print(\"Training Accuracy:\", train_accuracy_with_occlusion)\n",
        "print(\"Validation Accuracy:\", val_accuracy_with_occlusion)\n",
        "print(\"Training Loss:\", train_loss_with_occlusion)\n",
        "print(\"Validation Loss:\", val_loss_with_occlusion)\n",
        "\n",
        "# Comparison:\n",
        "print(\"\\nComparación de Resultados:\")\n",
        "\n",
        "# Compare final validation accuracies\n",
        "final_accuracy_no_occlusion = val_accuracy_no_occlusion[-1]\n",
        "final_accuracy_with_occlusion = val_accuracy_with_occlusion[-1]\n",
        "\n",
        "print(f\"- Precisión de validación final (sin oclusiones): {final_accuracy_no_occlusion:.4f}\")\n",
        "print(f\"- Precisión de validación final (con oclusiones): {final_accuracy_with_occlusion:.4f}\")\n",
        "\n",
        "if final_accuracy_no_occlusion > final_accuracy_with_occlusion:\n",
        "    print(\"- El modelo entrenado sin datos de oclusión obtuvo una precisión de validación mayor.\")\n",
        "elif final_accuracy_with_occlusion > final_accuracy_no_occlusion:\n",
        "    print(\"- El modelo entrenado con datos de oclusión obtuvo una precisión de validación mayor.\")\n",
        "else:\n",
        "    print(\"- Ambos modelos obtuvieron la misma precisión de validación.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3PYIEt7vziS",
        "outputId": "09048979-7da7-443a-a391-b32499cacc33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MobileNetV2 without Occlusion Data:\n",
            "Training Accuracy: [0.9998547434806824, 1.0, 1.0, 1.0, 1.0]\n",
            "Validation Accuracy: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Training Loss: [0.0007825368666090071, 0.0, 0.0, 0.0, 0.0]\n",
            "Validation Loss: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "\n",
            "MobileNetV2 with Occlusion Data:\n",
            "Training Accuracy: [0.938178300857544, 0.9498269557952881, 0.9540162682533264, 0.9589089155197144, 0.9625552892684937, 0.9673554301261902, 0.9732291102409363, 0.9786708950996399, 0.9819470643997192, 0.9852602481842041]\n",
            "Validation Accuracy: [0.9461007118225098, 0.94856858253479, 0.950345516204834, 0.9412142038345337, 0.950765073299408, 0.9437808394432068, 0.9390177726745605, 0.9457551836967468, 0.9481737613677979, 0.9410414695739746]\n",
            "Training Loss: [0.21774154901504517, 0.1427031010389328, 0.12529656291007996, 0.10942304879426956, 0.09754703938961029, 0.0828733742237091, 0.0689094290137291, 0.05764495208859444, 0.04833448678255081, 0.04111591726541519]\n",
            "Validation Loss: [0.15140192210674286, 0.1529390811920166, 0.13799943029880524, 0.15969917178153992, 0.15456731617450714, 0.1710156351327896, 0.19668014347553253, 0.2376745194196701, 0.3077927529811859, 0.2605472207069397]\n",
            "\n",
            "Comparación de Resultados:\n",
            "- Precisión de validación final (sin oclusiones): 1.0000\n",
            "- Precisión de validación final (con oclusiones): 0.9410\n",
            "- El modelo entrenado sin datos de oclusión obtuvo una precisión de validación mayor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparar la precisión de entrenamiento y validación de dos modelos MobileNetV2 (uno entrenado con oclusiones y otro sin ellas)"
      ],
      "metadata": {
        "id": "k_Zi2glO8iP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Función para predecir la clase de una imagen utilizando un modelo MobileNetV2, que clasifica entre \"Blurry,\" \"Eyeglasses,\" \"Hat\" y \"No Occlusion\""
      ],
      "metadata": {
        "id": "Sermi0Mw8y4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Función de predicción para la clasificación de imagen con todas las clases\n",
        "def predict_image_class(image_path):\n",
        "    img = image.load_img(image_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.  # Normalizar la imagen\n",
        "\n",
        "    prediction = mobilenetv2_model.predict(img_array)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "\n",
        "    # Actualizar con las etiquetas de clase\n",
        "    class_labels = {\n",
        "        0: 'Blurry',\n",
        "        1: 'Eyeglasses',\n",
        "        2: 'Hat',\n",
        "        3: 'No Occlusion'\n",
        "    }\n",
        "    predicted_label = class_labels.get(predicted_class, \"Clase desconocida\")\n",
        "\n",
        "    print(f\"Predicción para la imagen es: {predicted_label}\")\n",
        "    return predicted_label\n",
        "\n",
        "# Ejemplo de uso (reemplazar con tu ruta de imagen)\n",
        "image_path = '/root/.cache/kagglehub/datasets/jessicali9530/celeba-dataset/versions/2/img_align_celeba/img_align_celeba/000001.jpg' # Reemplazar con la ruta de tu imagen\n",
        "predicted_label = predict_image_class(image_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukx8L0b5suq7",
        "outputId": "70a52a97-2300-4ff8-d2f6-e4c7b4f9de3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Predicción para la imagen es: No Occlusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertir un modelo de Keras (en este caso, un modelo de MobileNetV2) a un formato TensorFlow Lite.\n",
        "Esto es esencial para facilitar el uso del modelo en dispositivos móviles donde se requiere un tamaño de modelo más pequeño y un tiempo de inferencia más rápido."
      ],
      "metadata": {
        "id": "7t3CJ_ozy_bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(mobilenetv2_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "s2VAqidF-TNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a807acc8-b8ff-43f6-8f30-69ee3b3e9d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpe4gytone'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_154')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  134786386317856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783203515504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783203511280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783203511632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783203522544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783188636368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783188636016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783188644112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783188633552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783188642176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783188647456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027444048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027441408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027441760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027442992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027446512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027446160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027449856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027447216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027448096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027454608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027454784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027523680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027455136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027453904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027527024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027526672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027530720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027528080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027528960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027533184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027532832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027536528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027533888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027534768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027555920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027556448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027558560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027557504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027558384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027563488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027563136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027567184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027564544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027565424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027570704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027639776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027638016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027570176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027637312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027644704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027644176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027648048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027645408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027646288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027651744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027686464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027688576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027652272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027651920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027690160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027689808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027693856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027691392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027691744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027697552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027692096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027700896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027698256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027699136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027737200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027736848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027740896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027738256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027739136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027744592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027744240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027747936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027745296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027746176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027751632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027804320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027801680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027802032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027802912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027807664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027807312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027811360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027808720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027809600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027815056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027815232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027933280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027815584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027813120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027938208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027937680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027941552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027938912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027939792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027945248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027933808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027945776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027945952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785027946832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259800880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259800528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259804576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259802112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259802464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259808272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259798944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259811616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259808976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259809856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259813904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259897424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259899184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259898304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259898656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259904464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259904112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259907808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259905168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259906048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259911504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259962608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259964720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259912032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259910096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259967536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259967184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259971232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259968592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259969472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259974928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259970352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259978272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259975632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785259976512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260031312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260030784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260034656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260032016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260032896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260038000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260037648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260041696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260039056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260039936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260043456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260109888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260114464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260112000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260112352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260118160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260117632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260121504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260118864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260119744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260125200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260210656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260208896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260124672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260208192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260213120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260212768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260216464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260213824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260214704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260220160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260208720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260223504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260220864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260221744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260325344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260324992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260329040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260326400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260327280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260332736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260328160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260336080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260333440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260334320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260336432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260372560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260374496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260373616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260373968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260379424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260379072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260383120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260380480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260381360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260386816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260454304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260456416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260388048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260454832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260460816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260460288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260464160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260461520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260462400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260467856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260468032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260536928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260468384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260465920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260539040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260538688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260542736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260540272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260540624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260546432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260540976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260549776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260547136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785260548016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031783168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031783696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031787216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031784752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031785104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031790912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031790560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031794256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031791616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031792496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031797952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031591664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783188637248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031587440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031589200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031600640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031670416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031668128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031669360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031667776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031673760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031673408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031677456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031674816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031675696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031681152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031672176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031681680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031681856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785031682736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785032195936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134786095383120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783203792800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134785032196992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783203790864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783205146144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783203518848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783203508816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783203519200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134783203511456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134779851078144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134779851382224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134779851074448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  134779851377296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    }
  ]
}